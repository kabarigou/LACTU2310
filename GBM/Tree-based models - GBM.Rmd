---
title: Gradient Boosting Model on Count Data
output: html_document
---

# Gradient Boosting Model

## Introduction

We can start by loading the packages.


```{r}
library(gbm)
library(caret)
library(mgcv)
library(reshape2)
library(ggplot2)
library(dplyr)
library(arrow)
```

We can also load the data (same as in the previous sessions).

```{r}
dataset = read_parquet(file = "../data/dataset.parquet")

set.seed(21)
in_training = createDataPartition(dataset$ClaimNb, times = 1, p = 0.8, list = FALSE)
training_set = dataset[in_training, ]
testing_set = dataset[-in_training, ]
```

## R Package GBM

We will use the package gbm available on CRAN. This Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart).

### Small Example with two variables.

The main function is gbm.

```{r}
set.seed(1)
m0 = gbm(ClaimNb ~ offset(log(Exposure)) + CarAge+DriverAge,
         data = training_set,
         distribution = "poisson",
         interaction.depth = 5, #the number of splits considered in each tree 
         n.trees = 50,
         shrinkage = 0.1,#Default is 0.1
         bag.fraction = 0.5, # At each tree, only 50% of the data is considered
         keep.data = TRUE,
         verbose = TRUE
) 

```

We can find the optimal number of trees using gbmt_performance. In this case, since no validation set has been provided, it will be based on out-of-bag samples. Does this make sense ? What is an OOB in the case of a GBM ?

```{r}
best.iter <- gbm.perf(m0, method = "OOB")
print(best.iter)
```

Sometimes the GBM won’t have enough trees. This can be seen when the optimal number of trees equals to total number of trees. No need to rerun everything, we can just add some new trees.

```{r}
set.seed(2)
m1 = gbm.more(m0, n.new.trees = 400, verbose = TRUE)
```

We can check the optimal number of trees based on the out-of-bag observations.

```{r}
gbm.perf(m1, method = "OOB")
print(best.iter)
```

**However, since OOB does not make much sense for GBM (due to the sequential nature of the algorithm), one should rather rely on cross-validation to find the optimal number of boosting iterations.**

```{r}
set.seed(10)
m0 = gbm(ClaimNb ~ offset(log(Exposure)) + CarAge+DriverAge,
         data = training_set,
         distribution = "poisson",
         interaction.depth = 5, #the number of splits considered in each tree 
         n.trees = 400,
         shrinkage = 0.01,#Default is 0.1
         bag.fraction = 0.5, # At each tree, only 50% of the data is considered
         keep.data = TRUE,
         verbose = TRUE,
         cv.folds = 5,
         n.cores=8
) 
```

Here, with cv, unfortunately, we cannot add more trees. We can plot the deviance as a function of the number of trees: a "maximalist" approach should be preferred. We can still prune the GBM, but not add extra boosting steps.


```{r}
# Check performance using 5-fold cross-validation
best.iter <- gbm.perf(m0, method = "cv")
```

To get the numerical value,

```{r}
best.iter <- gbm.perf(m0, method = "cv")
print(best.iter)
```

The cross-validation error for the optimal iteration can be retrieved.

```{r}
m0$cv.error[gbm.perf(m0, method = "cv")]
```

We also have a variable importance metric. For GBMs, importance depends on whether that variable was selected to split on during the tree building process, and how much the loss function (over all trees) improved (decreased) as a result.

```{r}
summary(m0, n.trees = best.iter)
```

We can plot the partial dependencies of the variables.

```{r}
par(mfrow = c(2, 1))
plot(m0, i.var = 1, n.trees = best.iter, type = "response")
plot(m0, i.var = 2, n.trees = best.iter, type = "response")
```

When we are finished, we can evaluate the performance of the model on the testing set.

```{r}
2 * (sum(dpois(x = testing_set$ClaimNb, lambda = testing_set$ClaimNb, log = TRUE)) -
    sum(dpois(x = testing_set$ClaimNb, lambda = predict(m0, 
                                                        newdata = testing_set,
                                                        n.trees = best.iter, 
                                                        type = "response") * testing_set$Exposure, log = TRUE)))
```

We can take a look at the first tree, using the pretty_gbm_tree function.

```{r}
pretty.gbm.tree(m0, i.tree =1) %>% 
  filter(SplitVar != -1) # Only keep non terminal nodes
#The indices are 0 for first term, 1 for second term, etc.
```

### Using all variables

Let’s now perform the cross-validation with 1000 trees. (~ 6 minutes). We will only perform this CV for one set of hyperparameters (due to time constraint). Obviously, the idea would be to *loop* on different values for these hyperparameters, and select those that yield the lowest loss error as estimated by cross-validation.

```{r}
set.seed(89)
m0_gbm = gbm(ClaimNb ~ offset(log(Exposure)) + CarAge + DriverAge + Power +Brand + Gas + Region + Density,
         data = training_set,
         distribution = "poisson",
         interaction.depth = 5, #the number of splits considered in each tree 
         n.trees = 1000,
         shrinkage = 0.01,#Default is 0.1
         n.minobsinnode = 1000,
         bag.fraction = 0.5, # At each tree, only 50% of the data is considered
         keep.data = TRUE,
         verbose = TRUE,
         cv.folds = 5,
         n.cores=8
) 
```

```{r}
best.iter <- gbm.perf(m0_gbm, method = "cv")
```

Let us store the optimal number of iterations.

```{r}
best.iter <- gbm.perf(m0_gbm, method = "cv")
best.iter
```

We can see the variable importance.

```{r}
summary(m0_gbm)
```

Another visualization:

```{r}
var.imp <- summary(m0_gbm, plotit=FALSE)

# Create the data frame based on provided values
imp <- data.frame(
  var = var.imp[1:7,1],
  rel_inf = var.imp[1:7,2]
)

# Generate the plot
ggplot(imp, aes(x = reorder(var, rel_inf), y = rel_inf)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(title = "Variable Importance", x = "Variable", y = "Relative Influence (%)") +
  theme_minimal()
```


We can take a look at the partial dependencies.

```{r}
par(mfrow = c(2, 4))
for (j in 1:7) {
    plot(m0_gbm, i.var = j, n.trees = best.iter, type = "response")
}
```

#### Interactions

Let’s compute Friedman’s H statistic for interaction, for all the possible couple of variable.
Friedman's H statistic allows to identify which variables appear to be in interaction. This is typically a good way to identify interactions for a simpler model, such as a GLM.

```{r}
var_names = c("CarAge", "DriverAge", "Power", "Brand", "Gas", "Region", "Density")

res = matrix(NA, 7, 7)
for (i in 1:6) {
    for (j in (i + 1):7) {
        res[i, j] = interact.gbm(m0_gbm, data = training_set, i.var = c(i,j), best.iter)
    }
}
diag(res) = 0
row.names(res) = var_names
colnames(res) = row.names(res)

interact_melt <- melt(res, na.rm = TRUE)

ggplot(data = interact_melt, aes(x = Var1, y = Var2, fill = value)) + geom_tile(color = "white") +
    scale_fill_gradient2(low = "white", mid = "gray", high = "blue", name = "Friedman's\nH-statistic") +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, size = 12, hjust = 1)) +
    coord_fixed()
```

We can inspect some of the partial dependencies.

```{r}
plot(m0_gbm, i.var = c(which(var_names == "Gas"), which(var_names == "Power")),
    n.trees = best.iter, type = "response")  #Power, Gas
```

```{r}
plot(m0_gbm, i.var  = c(which(var_names == "Gas"), which(var_names == "Region")),
    n.trees = best.iter, type = "response")  # Gas, Region
```

```{r}
plot(m0_gbm,  i.var = c(which(var_names == "Brand"), which(var_names == "CarAge")),
    n.trees = best.iter, type = "response")
```

```{r}
plot(m0_gbm,  i.var = c(which(var_names == "Power"), which(var_names == "Brand")),
   n.trees = best.iter, type = "response")  # Gas, Region
```

```{r}
plot(m0_gbm,  i.var = c(which(var_names == "CarAge"), which(var_names == "DriverAge")),
    n.trees = best.iter, type = "response")  # Gas, Region
```

We can check the prediction power of the model on the testing set.

```{r}
2 * (sum(dpois(x = testing_set$ClaimNb, lambda = testing_set$ClaimNb, log = TRUE)) -
    sum(dpois(x = testing_set$ClaimNb, lambda = predict(m0_gbm, newdata = testing_set,
        n.trees = best.iter, type = "response") * testing_set$Exposure, log = TRUE)))
```

#### Tweaking the model

We would have to tweak the parameters. We should define a grid of parameters and perform cross-validation to choose the optimal parameters. Due to time restriction, we will only show one example, for instance (interaction depth changed from 5 to 10):

```{r}
set.seed(89)
m1_gbm = gbm(ClaimNb ~ offset(log(Exposure)) + CarAge + DriverAge + Power +Brand + Gas + Region + Density,
         data = training_set,
         distribution = "poisson",
         interaction.depth = 10, #the number of splits considered in each tree 
         n.trees = 1000,
         shrinkage = 0.01,#Default is 0.1
         n.minobsinnode = 1000,
         bag.fraction = 0.5, # At each tree, only 50% of the data is considered
         keep.data = TRUE,
         verbose = TRUE,
         cv.folds = 5,
         n.cores=8
) 
```

We need to identify the optimal number of boosting iterations...

```{r}
best.iter=gbm.perf(m1_gbm, method = "cv")
best.iter
```

Let’s compare both cross-validation errors.

```{r}
c(min(m0_gbm$cv.error),min(m1_gbm$cv.error))
```

The difference seems small… however, if we look at the predictions..

```{r}
require(ggplot2)
ggplot() + 
  geom_histogram(aes(x=predict(m0_gbm,newdata = training_set,n.trees=best.iter, type="response") / 
                       predict(m1_gbm,newdata = training_set,n.trees=best.iter, type="response") -1),
                 bins=60) + 
  xlab("Relative Difference") + ggtitle("Relative Difference between both models")
```

In pratice, this means that two actuaries perfoming prediction modelling on the same data are actually very unlikely to obtain for single policies similar predictions, although they might end up with an overall similar loss.

### Intercept of the GBM

It may be required to correct the “intercept” of a gbm. Indeed, let us take at look at the total expected number of claims and compare it to the observed one. Let us do the same comparison with the GAM from above.

```{r}
pred = data.frame(gbm = predict(m0_gbm,
                                newdata = training_set, 
                                n.trees = best.iter, 
                                type = "response") * training_set$Exposure)

print(paste(c("GBM predicted ", "GBM  obs"),c(sum(pred$gbm), sum(training_set$ClaimNb))))
```

GBM tend to underestimate the total number of claims. We could correct the claim frequency with

```{r}
intercept_correction = sum(training_set$ClaimNb)/sum(pred$gbm)
intercept_correction
```

Let us do it, and recompute the error on the testing set, first without correction, then with correction.

```{r}
# Without correction

2*(sum(dpois(x = testing_set$ClaimNb, lambda = testing_set$ClaimNb,log=TRUE))-
  sum(dpois(x = testing_set$ClaimNb, lambda = predict(m0_gbm,newdata = testing_set,n.trees=best.iter, type="response") * testing_set$Exposure,
            log=TRUE)))
```

```{r}
# With correction
2*(sum(dpois(x = testing_set$ClaimNb, lambda = testing_set$ClaimNb,log=TRUE))-
  sum(dpois(x = testing_set$ClaimNb, lambda = intercept_correction * predict(m0_gbm,newdata = testing_set,n.trees=best.iter, type="response") * testing_set$Exposure,
            log=TRUE)))
```

## Package LightGBM

Other packages than the original R package GBM (or the improved version GBM3) exist now. We can cite XGBoost, LightGBM and Catboost for instance.
In the next section, we will investigate LightGBM (https://lightgbm.readthedocs.io/en/latest/index.html)

Lightgbm has originally been developped by Microsoft, is free and open-source. It is usable in many languages such as Python and R. The original algorithm has been adapted with the final objective to be *light* and *fast* (e.g. https://lightgbm.readthedocs.io/en/latest/Features.html#optimization-in-speed-and-memory-usage)

```{r}
if (!require('lightgbm')) {install.packages("lightgbm")}
require(lightgbm)
```

To use LightGBM, we need to create a 'lgb.Dataset' object. Note that the data argument does not accept dataframes. We used data.matrix to convert the dataframe to a matrix. Factors are converted to numbers. We can specify which columns relate to categorical features.

```{r}
cols = c('CarAge', 'DriverAge', 'Density', 'Power', 'Brand','Gas', 'Region')
lgb_train = lgb.Dataset(data = data.matrix(training_set[cols]), 
                        label = training_set$ClaimNb,
                        categorical_feature = c(4,5,6,7),
                        init_score = log(training_set$Exposure)
                       )
```

We can call the lightgbm function to estimate our model. The list of parameters we can play with is listed on the website (rather than the R documentation which does not list them all): https://lightgbm.readthedocs.io/en/latest/Parameters.html

```{r}
set.seed(42)
nrounds = 1000

params <- list(
  learning_rate = 0.1,
  max_depth = 10,
  bagging_fraction = 0.5,
  feature_fraction = 1,
  feature_fraction_bynode = 1,
  objective = "poisson",   
  metric = "poisson"        
)

lgb_m0 <- lgb.cv(
  data = lgb_train,
  params = params,
  nrounds = nrounds,
  verbose = 1L,
  eval_freq = 1L,
  nfold = 5
)
```

```{r}
best.iter = lgb_m0$best_iter
best.iter
```

```{r}
best.score = lgb_m0$best_score
best.score
```

We can try to tweak the model by changing part of its hyperparameters (again, a gridsearch would be better). Here we are going to search for the optimal max_depth parameter

We could for instance, try to find the optimal max_depth parameter e.g. a range from 2 to 10, and two possibles values for the bagging fraction and the feature fraction selection by node.

```{r}
list_hyperparameters = expand.grid(
    learning_rate = 0.1,
    max_depth = seq(2,10,1),
    bagging_fraction = c(0.5, 1),
    feature_fraction = 1,
    feature_fraction_bynode = c(0.5,1),
    score = 0,
    best_iter = 0
)
```

```{r}
row = 1
list_hyperparameters[row, 'learning_rate']
```

```{r}
for (row in seq(nrow(list_hyperparameters))) {
  
  # Build the parameter list for the current iteration.
  # Note that objective and metric are now specified inside the params list.
  params <- list(
    learning_rate = list_hyperparameters[row, "learning_rate"],
    max_depth = list_hyperparameters[row, "max_depth"],
    bagging_fraction = list_hyperparameters[row, "bagging_fraction"],
    feature_fraction = list_hyperparameters[row, "feature_fraction"],
    feature_fraction_bynode = list_hyperparameters[row, "feature_fraction_bynode"],
    monotone_constraints = c(0, 0, 0, 1, 0, 0, 0),
    objective = "poisson",
    metric = "poisson"
  )

  # Perform cross-validation using lgb.cv with the current set of parameters.
  lgb_m1 <- lgb.cv(
    data = lgb_train,
    params = params,
    nrounds = 1000,
    verbose = 0,
    eval_freq = 1L,
    nfold = 5
  )
  
  # Extract the best iteration and best score from the cross-validation results.
  current.iter <- lgb_m1$best_iter
  current.score <- lgb_m1$best_score
  
  # Save the results back into the hyperparameters data frame.
  list_hyperparameters[row, "score"] <- current.score
  list_hyperparameters[row, "best_iter"] <- current.iter
}
```

```{r}
list_hyperparameters
```

```{r}
optimal_row = which.min(list_hyperparameters$score)
best_iter = list_hyperparameters[optimal_row,"best_iter"]
learning_rate = list_hyperparameters[optimal_row,"learning_rate"]
max_depth = list_hyperparameters[optimal_row,"max_depth"]
bagging_fraction = list_hyperparameters[optimal_row,"bagging_fraction"]
feature_fraction = list_hyperparameters[optimal_row,"feature_fraction"]
feature_fraction_bynode = list_hyperparameters[optimal_row,"feature_fraction_bynode"]
list_hyperparameters[optimal_row,]
```

Re-estimate the whole model with the optimal hyperparameters

```{r}
lgb_params = list(
  learning_rate = learning_rate,
  max_depth = max_depth,
  bagging_fraction = bagging_fraction,
  feature_fraction = feature_fraction,
  feature_fraction_bynode = feature_fraction_bynode,
  monotone_constraints = c(0, 0, 0, 1, 0, 0, 0),
  objective = "poisson",
  eval = "poisson"
)

lgb_m1_final = lightgbm(
  data = lgb_train,
  params = lgb_params,
  nrounds = best_iter,
  verbose = 1L,
  eval_freq = 1L
)
```

```{r}
pred = predict(lgb_m1_final, data.matrix(testing_set[cols]))

2*(sum(dpois(x = testing_set$ClaimNb, 
             lambda = testing_set$ClaimNb,
             log=TRUE))-
  sum(dpois(x = testing_set$ClaimNb, 
            lambda = pred * testing_set$Exposure, 
            log=TRUE)))
```

```{r}
tree_imp = lgb.importance(lgb_m1_final, percentage = TRUE)
lgb.plot.importance(tree_imp, top_n = 5L, measure = "Gain")
```

